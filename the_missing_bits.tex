% !TeX root = ./main.tex

\chapter*{拾遗}
\addcontentsline{toc}{chapter}{拾遗}

为了简洁起见，本书跳过了许多重要主题，特别是：

\subsubsection*{循环神经网络}

在注意力模型展现出更高性能之前，\keyterm{递归神经网络}（\keyterm{RNN}）是处理文本或声音样本等时间序列数据的标准方法。这些架构拥有一个内部隐藏状态，每当序列的一个组成部分被处理时就会更新。它们的主要组成部分是 LSTM \citep{lstm} 或 GRU \citep{arxiv-1406.1078} 这样的层。

训练一个递归架构相当于在时间上展开它，这会导致一长串的操作符组合。在历史上这促进了现在被广泛应用于深度框架的关键技术的设计，比如\keyterm{整流器}和门控，一种动态调制的\keyterm{跳跃连接}形式。

\subsubsection*{自编码器}

\keyterm{自编码器}是一种模型，它将高维输入信号映射到低维的潜在表示，然后再将其映射回原始信号，确保信息得以保存。我们在 \ref{sec6.1} 节中看到了其用于去噪的应用，但它也可用于自动发现数据流形的有意义的低维参数化。

由 \cite{arxiv-1312.6114} 提出的\keyterm{变分自编码器}（\keyterm{VAE}）是一种具有相似结构的生成模型。它通过损失函数，对潜在表示施加一个预定义的分布。这样，在训练之后，通过按照这个施加的分布对潜在表示进行采样，然后通过解码器映射回去，便可以生成新的样本。

\subsubsection*{生成对抗网络}

另一种密度建模方法是 \cite{arxiv-1406.2661} 提出的\keyterm{生成对抗网络}（\keyterm{GAN}）。该方法结合了一个\keyterm{生成器}和一个\keyterm{鉴别器}，生成器采用遵循固定分布的随机输入作为输入，并生成图像等结构化信号，鉴别器采用样本作为输入并预测它是来自训练集还是由生成器生成。

训练通过最小化标准交叉熵损失来优化判别器，并通过最大化判别器的损失来优化生成器。可以证明，在平衡状态下，生成器产生的样本与真实数据无法区分。实际操作中，当梯度通过判别器流向生成器时，它会告知后者需要解决的判别器所使用的线索。

\subsubsection*{图神经网络}

许多应用需要处理那些并非规则排列在网格上的信号。例如，蛋白质、三维网格、地理位置或社交互动这类数据可以更自然地以图结构来呈现。标准卷积网络甚至注意力模型都不适合处理此类数据，而此类任务的首选工具是\keyterm{图神经网络}（\keyterm{GNN}）\citep{gnn}。

这些模型由多层组成，它们通过线性组合每个顶点的直接相邻顶点上的激活来计算每个顶点处的激活。这种操作与标准卷积非常相似，除了数据结构不反映与其携带的特征向量相关的任何几何信息。

\subsubsection*{自监督训练}

如 \ref{sec7.1} 节所述，即便大语言模型仅被训练用来预测下一个单词，但在大规模未标记数据集上训练的大语言模型，如 GPT（参见 \ref{sec5.3} 节）能够解决各种任务，例如识别单词的语法角色、回答问题，甚至从一种语言翻译成另一种语言 \citep{Radford2019}。

这类模型构成了一大类方法，此类方法统称为\keyterm{自监督学习}，旨在尝试利用未标记的数据集 \citep{arxiv-2304.12210}。

这些方法的关键原则是定义一个不需要标签但需要对感兴趣的实际任务有用的特征表示的任务，对于这个任务，存在一个小型标记数据集。例如，在计算机视觉中，图像特征可以被优化，使其对不改变图像语义内容的数据变换保持不变，同时在统计上不相关 \citep{arxiv-2103.03230}。

在 NLP 和计算机视觉中，一个强大的通用策略是训练模型来恢复被屏蔽的部分信号 \citep{arxiv-1810.04805, arxiv-2111.07832}。